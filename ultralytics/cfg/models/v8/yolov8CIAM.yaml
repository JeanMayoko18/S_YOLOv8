# Ultralytics YOLO ðŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect

# Parameters
nc: 88  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs

# The CIAM module is important because it enhances the contextual reasoning capabilities of object detection models, leading to improved performance and accuracy. Here's a breakdown of its key benefits:

#    Enhanced Contextual Information:
#       CIAM aggregates features from multiple scales of the feature pyramid, enriching the feature representation with contextual information from a wider receptive field. This helps the model better understand the relationship between objects and their surroundings.
#   Improved Detection Accuracy:
#        By incorporating contextual cues, CIAM helps in resolving ambiguities and reducing false positives, ultimately improving the accuracy of object detection. It allows the model to make more informed decisions about the presence and location of objects.
#   Better Object Recognition in Complex Scenes:
#        CIAM enables the model to recognize objects even in cluttered or challenging scenes where contextual information is crucial for accurate detection. It helps in differentiating between similar-looking objects or objects that are partially occluded.
#  Robustness to Occlusion and Noise:
#       The multi-scale feature aggregation and transformer encoder in CIAM provide a degree of robustness against occlusions and noise. By considering a wider range of features, the model can still detect objects even if parts of them are obscured or corrupted by noise.

#Overall, CIAM plays a crucial role in improving the performance and robustness of object detection models by enhancing their contextual reasoning capabilities. This makes it particularly valuable in applications where complex scenes and challenging conditions are encountered.

#In the context of YOLOv8, integrating the CIAM module can potentially lead to significant improvements in detection accuracy and generalization ability, especially in scenarios with crowded or complex scenes.

#I Potential CIAM Integration Points

#    Within the Backbone:

 #       After C2f Blocks: Inserting CIAM after the C2f blocks (indices 2, 4, 6, 8 in the backbone) could enhance feature representation before downsampling.

#        Before the SPPF Module: Adding CIAM before the SPPF module (index 9) could refine features before the final spatial pyramid pooling operation.

#    Within the Head:

#        After Concatenation: Introducing CIAM after the concatenation operations (indices 11, 14, 17, 20) could help the network focus on the fused features from different scales.

#        Before the Detect Layer: Placing CIAM before the Detect layer (index 22) could further refine features before object detection.


# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2fRFEM, [128, True]] # 2
  - [-1, 1, CIAM, [32, 8, 256]] # 3 - Added CIAM
  - [-1, 1, Conv, [256, 3, 2]]  # 4-P3/8
  - [-1, 6, C2fRFEM, [256, True]] #5
  - [-1, 1, Conv, [512, 3, 2]]  # 6-P4/16
  - [-1, 6, C2fRFEM, [512, True]] #7
  - [-1, 1, Conv, [1024, 3, 2]]  # 8-P5/32
  - [-1, 3, C2fRFEM, [1024, True]] #9
  - [-1, 1, SPPF, [1024, 5]]  # 10

  
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']] #11
  - [[-1, 7], 1, Concat, [1]]  # 12 cat backbone P4, index adjusted to 7 instead of 6
  - [-1, 3, C2f, [512]]  # 13, index incremented

  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 5], 1, Concat, [1]]  #15 cat backbone P3, index adjusted instead of 4
  - [-1, 3, C2f, [256]]  # 16 (P3/8-small)

  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 13], 1, Concat, [1]]  # cat head P4, index adjusted instead of 12
  - [-1, 3, C2f, [512]]  # 19 (P4/16-medium) was 18

  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 10], 1, Concat, [1]]  # cat head P5, index adjusted to 10 instead of 9
  - [-1, 3, C2f, [1024]]  # 22 (P5/32-large) was 21

  - [[16, 19, 22], 1, Detect, [nc]]  # Detect(P3, P4, P5)
